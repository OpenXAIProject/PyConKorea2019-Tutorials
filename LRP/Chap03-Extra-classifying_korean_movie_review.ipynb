{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        # txt 파일의 헤더(id document label)는 제외하기\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "train_data = read_data('nsmc/ratings_train.txt')\n",
    "test_data = read_data('nsmc/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "3\n",
      "50000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_data[0]))\n",
    "print(len(test_data))\n",
    "print(len(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohee/.local/lib/python3.5/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Noun'), ('밤', 'Noun'), ('그날', 'Noun'), ('의', 'Josa'), ('반딧불', 'Noun'), ('을', 'Josa'), ('당신', 'Noun'), ('의', 'Josa'), ('창', 'Noun'), ('가까이', 'Noun'), ('보낼게요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "print(okt.pos(u'이 밤 그날의 반딧불을 당신의 창 가까이 보낼게요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['아/Exclamation',\n",
      "  '더빙/Noun',\n",
      "  '../Punctuation',\n",
      "  '진짜/Noun',\n",
      "  '짜증나네요/Adjective',\n",
      "  '목소리/Noun'],\n",
      " '0')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "# if os.path.isfile('nsmc/train_docs.json'):\n",
    "#     with open('nsmc/train_docs.json') as f:\n",
    "#         train_docs = json.load(f)\n",
    "#     with open('nsmc/test_docs.json') as f:\n",
    "#         test_docs = json.load(f)\n",
    "# else:\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
    "# JSON 파일로 저장\n",
    "with open('nsmc/train_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "    json.dump(train_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "with open('nsmc/test_docs.json', 'w', encoding=\"utf-8\") as make_file:\n",
    "    json.dump(test_docs, make_file, ensure_ascii=False, indent=\"\\t\")\n",
    "\n",
    "# 예쁘게(?) 출력하기 위해서 pprint 라이브러리 사용\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159921\n"
     ]
    }
   ],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: NMSC>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens, name='NMSC')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159921\n",
      "104812\n",
      "[('./Punctuation', 67778),\n",
      " ('영화/Noun', 50818),\n",
      " ('이/Josa', 38540),\n",
      " ('의/Josa', 30188),\n",
      " ('../Punctuation', 29055),\n",
      " ('가/Josa', 26627),\n",
      " ('에/Josa', 26468),\n",
      " ('을/Josa', 23118),\n",
      " ('.../Punctuation', 22795),\n",
      " ('도/Josa', 20037)]\n"
     ]
    }
   ],
   "source": [
    "# 전체 토큰의 개수\n",
    "print(len(text.tokens))\n",
    "\n",
    "# 중복을 제외한 토큰의 개수\n",
    "print(len(set(text.tokens)))            \n",
    "\n",
    "# 출현 빈도가 높은 상위 토큰 10개\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = [f[0] for f in text.vocab().most_common(9999)]\n",
    "\n",
    "def term_frequency(doc):\n",
    "    return [doc.count(word) for word in selected_words]\n",
    "\n",
    "train_x = [term_frequency(d) for d, _ in train_docs]\n",
    "test_x = [term_frequency(d) for d, _ in test_docs]\n",
    "train_y = [c for _, c in train_docs]\n",
    "test_y = [c for _, c in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = [f[0] for f in text.vocab().most_common(9999)]\n",
    "\n",
    "#train data\n",
    "train_x = []\n",
    "for i in range(len(train_docs)):\n",
    "    _ = []\n",
    "    for word in train_docs[i][0] :\n",
    "        try : \n",
    "            _.append(selected_words.index(word))\n",
    "        except :\n",
    "            _.append(9999)\n",
    "    train_x.append(_)\n",
    "    \n",
    "#test data\n",
    "test_x = []\n",
    "for i in range(len(test_docs)):\n",
    "    _ = []\n",
    "    for word in test_docs[i][0] :\n",
    "        try : \n",
    "            _.append(selected_words.index(word))\n",
    "        except :\n",
    "            _.append(9999)\n",
    "    test_x.append(_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words.append('***/') # 10000위 안에 들지 못하는 단어들은 모두 ***라고 표기함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'반가운/Adjective'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_words[9998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'***/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_words[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"nsmc_selected_words.txt\", \"wb\") as fp:\n",
    "    pickle.dump(selected_words, fp)  \n",
    "with open(\"nsmc_train_x.txt\", \"wb\") as fp:\n",
    "    pickle.dump(train_x, fp)\n",
    "with open(\"nsmc_test_x.txt\", \"wb\") as fp:\n",
    "    pickle.dump(test_x, fp)\n",
    "with open(\"nsmc_train_y.txt\", \"wb\") as fp:\n",
    "    pickle.dump(train_y, fp)\n",
    "with open(\"nsmc_test_y.txt\", \"wb\") as fp:\n",
    "    pickle.dump(test_y, fp)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
